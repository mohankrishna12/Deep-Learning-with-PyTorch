{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a conda environment:\n",
    "\n",
    "<code> conda create --name dlPyTorch </code>\n",
    "\n",
    "Activate the conda environment:\n",
    "\n",
    "<code> conda activate dlPyTorch </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[0.6582, 0.8258],\n",
      "        [0.2301, 0.3184]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.rand(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code> torch.cuda.is_available() </code> \n",
    "\n",
    "Here, the above line should return **TRUE** : PyTorch detects Graphics card. \n",
    "                                   **FALSE**: Need to reverify the Cuda installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "A simple scalar (e.g., 1) can be represented as a tensor of rank 0, a vector is rank 1, \n",
    "an n × n matrix is rank 2, and so on. In the previous example, we created a rank 2\n",
    "tensor with random values by using torch.rand()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change an element in a tensor by using standard Python indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 9],\n",
       "        [0, 1, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][3]=9\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special creation of functions to generate particular types of tensors. In particular, ones() and zeroes() will generate tensors filled\n",
    "with 1s and 0s, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.zeros(4,4)\n",
    "z=torch.ones(4,4)\n",
    "\n",
    "print(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some mathematical operations of tensors, shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[ 2.,  1.,  1., 10.],\n",
      "        [ 1.,  2.,  1.,  1.],\n",
      "        [ 1.,  1.,  2.,  1.],\n",
      "        [ 1.,  1.,  1.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "print(y+z)\n",
    "print(x+z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you have a tensor of rank 0 (a scalar), you can pull out the value with ***item()***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22166800498962402"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can live in the CPU or on the GPU and can be copied between\n",
    "devices by using the to() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_tensor = x\n",
    "cpu_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tensor = cpu_tensor.to(\"cuda\")\n",
    "gpu_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we often need to find the maximum item in a tensor as well as the\n",
    "index that contains the maximum value (as this often corresponds to the\n",
    "class that the neural network has decided upon in its final prediction). These\n",
    "can be done with the ***max()*** and ***argmax()*** functions. We can also use\n",
    "item() to extract a standard Python value from a 1D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4062, 0.1836, 0.3082],\n",
      "        [0.8257, 0.2483, 0.1372],\n",
      "        [0.4104, 0.8930, 0.2575]])\n",
      "tensor(0.8930)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8929583430290222"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=torch.rand(3,3)\n",
    "print(p)\n",
    "print(p.max())\n",
    "p.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the type of a tensor; for example, from a\n",
    "LongTensor to a FloatTensor. We can do this with ***to()***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=torch.tensor([[1,2],[3,4]]).to(dtype=torch.float32)\n",
    "print(m)\n",
    "m.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, functions that modify tensors come in two forms:\n",
    "\n",
    "1. Out-of-place (regular) functions\n",
    "These create a new tensor while keeping the original tensor unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1387, 0.0892],\n",
      "        [0.6188, 0.9524]])\n",
      "tensor([[-2.8505, -3.4870],\n",
      "        [-0.6924, -0.0703]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.rand(2,2)\n",
    "new_tensor = random_tensor.log2()  # Creates a new tensor\n",
    "print(random_tensor)  # Original tensor remains unchanged\n",
    "print(new_tensor)  # New tensor with log2 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In-place functions (with _ suffix)\n",
    "These modify the original tensor directly to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8505, -3.4870],\n",
      "        [-0.6924, -0.0703]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor.log2_()  # Modifies random_tensor in-place\n",
    "print(random_tensor)  # Now holds log2 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common operation is reshaping a tensor. This can often occur\n",
    "because your neural network layer may require a slightly different input\n",
    "shape than what you currently have to feed into it. \n",
    "For example, the\n",
    "Modified National Institute of Standards and Technology (MNIST) dataset\n",
    "of handwritten digits is a collection of 28 × 28 images, but the way it’s\n",
    "packaged is in arrays of length 784. To use the networks we are\n",
    "constructing, we need to turn those back into 1 × 28 × 28 tensors (the\n",
    "leading 1 is the number of channels—normally red, green, and blue—but as\n",
    "MNIST digits are just grayscale, we have only one channel). We can do this\n",
    "with either ***view()*** or ***reshape()***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_tensor = torch.rand(784)\n",
    "viewed_tensor = flat_tensor.view(1,28,28)\n",
    "viewed_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_tensor = flat_tensor.reshape(1,28,28)\n",
    "reshaped_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you might need to rearrange the dimensions of a tensor. You will\n",
    "likely come across this with images, which often are stored as **[height,\n",
    "width, channel]** tensors, but PyTorch prefers to deal with these in a\n",
    "**[channel, height, width]**. You can user permute() to deal with these\n",
    "in a fairly straightforward manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 480])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwc_tensor = torch.rand(640, 480, 3)\n",
    "chw_tensor = hwc_tensor.permute(2,0,1)\n",
    "chw_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we’ve just applied permute to a **[640,480,3]** tensor, with the\n",
    "arguments being the indexes of the tensor’s dimensions, so we want the\n",
    "final dimension (2, due to zero indexing) to be at the front of our tensor,\n",
    "followed by the remaining two dimensions in their original order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Broadcasting\n",
    "\n",
    "Borrowed from NumPy, broadcasting allows you to perform operations\n",
    "between a tensor and a smaller tensor. You can broadcast across two tensors\n",
    "if, starting backward from their trailing dimensions:\n",
    "\n",
    "1. The two dimensions are equal.\n",
    "\n",
    "2. One of the dimensions is 1.\n",
    "\n",
    "In our use of broadcasting, it works because 1 has a dimension of 1, and as\n",
    "there are no other dimensions, the 1 can be expanded to cover the other\n",
    "tensor. If we tried to add a **[2,2]** tensor to a **[3,3]** tensor, we’d get this\n",
    "error message:\n",
    "`The size of tensor a (2) must match the size of\n",
    "tensor b (3) at non-singleton dimension 1`\n",
    "But we could add a **[1,3]** tensor to the **[3,3]** tensor without any trouble.\n",
    "Broadcasting is a handy little feature that increases brevity of code, and is\n",
    "often faster than manually expanding the tensor yourself.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Broadcasting Rules\n",
    "To perform an operation between two tensors, PyTorch applies broadcasting if their shapes do not match exactly. The rules are:\n",
    "\n",
    "Trailing dimensions must either be equal or one of them must be 1.\n",
    "\n",
    "Missing dimensions in the smaller tensor are considered as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 7],\n",
      "        [6, 7, 8],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3]])  # Shape: [1, 3]\n",
    "B = torch.tensor([[4], [5], [6]])  # Shape: [3, 1]\n",
    "\n",
    "C = A + B  # Broadcasting happens here\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m A = torch.rand(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# Shape: [2, 2]\u001b[39;00m\n\u001b[32m      2\u001b[39m B = torch.rand(\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m)  \u001b[38;5;66;03m# Shape: [3, 3]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m C = \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m  \u001b[38;5;66;03m# RuntimeError: The size of tensor A (2) must match the size of tensor B (3)  \u001b[39;00m\n\u001b[32m      6\u001b[39m            \u001b[38;5;66;03m#[2, 2] cannot be broadcasted to [3, 3] because neither dimension is 1.\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "A = torch.rand(2, 2)  # Shape: [2, 2]\n",
    "B = torch.rand(3, 3)  # Shape: [3, 3]\n",
    "\n",
    "C = A + B  # RuntimeError: The size of tensor A (2) must match the size of tensor B (3)  \n",
    "\n",
    "           #[2, 2] cannot be broadcasted to [3, 3] because neither dimension is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(1, 3)  # Shape: [1, 3]\n",
    "B = torch.rand(3, 3)  # Shape: [3, 3]\n",
    "\n",
    "C = A + B  # Works! Broadcasting expands A to [3, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A expands to [3, 3]\n",
    "\n",
    "B remains [3, 3]\n",
    "\n",
    "Element-wise addition happens successfully.\n",
    "\n",
    "## Why use broadcasting?\n",
    "\n",
    "1. Efficient Computation\n",
    "2. Saves Memory\n",
    "3. Concise code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, We have introduced the fundamental building block of the\n",
    "library, the tensor. In the next notebook, to start building neural networks and classifying images, so one should be comfortable with tensors and their operations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
